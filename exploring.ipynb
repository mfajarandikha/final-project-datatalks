{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to youtube_live_streams.csv\n"
     ]
    }
   ],
   "source": [
    "from googleapiclient.discovery import build\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key from environment variables\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")\n",
    "CHANNEL_ID = \"UCCuzDCoI3EUOo_nhCj4noSw\"\n",
    "\n",
    "# Define start and end date\n",
    "START_DATE = \"2025-01-21T00:00:00Z\"\n",
    "END_DATE = \"2025-03-29T00:00:00Z\"\n",
    "\n",
    "\n",
    "# Function to fetch data from YouTube API and handle pagination\n",
    "def fetch_video_data(api_key, channel_id, start_date, end_date):\n",
    "    video_details = []\n",
    "    page_token = None  # To store the nextPageToken for pagination\n",
    "\n",
    "    while True:\n",
    "        # Construct the search URL\n",
    "        search_url = f\"https://www.googleapis.com/youtube/v3/search?part=snippet&channelId={channel_id}&type=video&eventType=completed&publishedAfter={start_date}&publishedBefore={end_date}&maxResults=50&key={api_key}\"\n",
    "\n",
    "        if page_token:\n",
    "            search_url += f\"&pageToken={page_token}\"\n",
    "\n",
    "        # Make the request to YouTube API\n",
    "        search_response = requests.get(search_url).json()\n",
    "\n",
    "        if \"items\" in search_response:\n",
    "            video_ids = [item[\"id\"][\"videoId\"] for item in search_response[\"items\"]]\n",
    "\n",
    "            # Get video details (statistics like views, likes, etc.)\n",
    "            stats_url = f\"https://www.googleapis.com/youtube/v3/videos?part=snippet,statistics&id={','.join(video_ids)}&key={api_key}\"\n",
    "            stats_response = requests.get(stats_url).json()\n",
    "\n",
    "            for item in stats_response.get(\"items\", []):\n",
    "                video_info = {\n",
    "                    \"title\": item[\"snippet\"][\"title\"],\n",
    "                    \"video_url\": f\"https://www.youtube.com/watch?v={item['id']}\",\n",
    "                    \"published_date\": item[\"snippet\"].get(\"publishedAt\", \"N/A\"),\n",
    "                    \"views\": item[\"statistics\"].get(\"viewCount\", \"N/A\"),\n",
    "                    \"likes\": item[\"statistics\"].get(\"likeCount\", \"N/A\"),\n",
    "                    \"comments\": item[\"statistics\"].get(\"commentCount\", \"N/A\"),\n",
    "                }\n",
    "                video_details.append(video_info)\n",
    "\n",
    "        # Check if there is another page of results\n",
    "        page_token = search_response.get(\"nextPageToken\")\n",
    "        if not page_token:\n",
    "            break  # No more pages to fetch\n",
    "\n",
    "    return video_details\n",
    "\n",
    "\n",
    "# Fetch the video data\n",
    "video_data = fetch_video_data(API_KEY, CHANNEL_ID, START_DATE, END_DATE)\n",
    "\n",
    "# Save the data to a CSV file\n",
    "if video_data:\n",
    "    df = pd.DataFrame(video_data)\n",
    "    df.to_csv(\"youtube_live_streams.csv\", index=False)\n",
    "    print(\"Data successfully saved to youtube_live_streams.csv\")\n",
    "else:\n",
    "    print(\"No archived live streams found within the given date range.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
